---
globs: lib/tool-wrapper.ts,lib/chat-service.ts
alwaysApply: false
---

# Tool Wrapper System

## Context

The tool wrapper system intercepts tool calls from the main LLM and wraps each tool execution with an LLM processing layer. The main model never sees raw tool results—instead, it receives processed, summarized insights generated by gpt-3.5-turbo.

**Status**: ✅ Fully implemented with logging

## Architecture

### Flow
1. Main model (gpt-4-turbo-preview) calls tool with `query` parameter
2. Tool wrapper executes HTTP tool call (mocked for now)
3. Wrapper sends raw results + query to gpt-3.5-turbo
4. gpt-3.5-turbo processes results and returns insights
5. Main model receives processed insights (never sees raw results)

### Key Files
- `lib/tool-wrapper.ts`: Tool execution wrapper with LLM processing
- `lib/chat-service.ts`: Tool conversion and execution handler

## Patterns

### Tool Definition Pattern
```typescript
// In lib/chat-service.ts
export function convertMethodsToOpenAITools(methods: Method[]) {
  return methods.map(method => ({
    type: "function" as const,
    function: {
      name: method.name,
      description: method.description || `Execute ${method.name} tool`,
      parameters: {
        type: "object" as const,
        properties: {
          query: {
            type: "string",
            description: "Natural language query describing what you want to know or do with this tool",
          },
        },
        required: ["query"],
      },
    },
  }));
}
```

### Tool Wrapper Pattern
```typescript
// In lib/tool-wrapper.ts
export async function executeToolWithLLMWrapper(
  method: Method,
  query: string
): Promise<string> {
  // 1. Execute HTTP tool call (mocked)
  const rawResult = await executeHttpTool(method);
  
  // 2. Prepare prompts for LLM processing
  const systemPrompt = `You are a tool execution assistant. 
    Analyze results and provide insights based on the user's query.
    Return only the processed insight as plain text.`;
  
  const userPrompt = `Tool: ${method.name}
    Description: ${method.description}
    User Query: "${query}"
    Tool Execution Result: ${JSON.stringify(rawResult)}
    
    Please provide insights that address the user's query.`;
  
  // 3. Call gpt-3.5-turbo to process results
  const response = await openai.chat.completions.create({
    model: "gpt-3.5-turbo",
    messages: [
      { role: "system", content: systemPrompt },
      { role: "user", content: userPrompt },
    ],
    temperature: 0.7,
  });
  
  return response.choices[0]?.message?.content || "Error processing results";
}
```

### Tool Execution Handler Pattern
```typescript
// In lib/chat-service.ts
if (assistantMessage.tool_calls && assistantMessage.tool_calls.length > 0) {
  // Execute all tool calls in parallel
  const toolCallPromises = assistantMessage.tool_calls.map(async (call) => {
    const toolName = call.function.name;
    const args = JSON.parse(call.function.arguments);
    const query = args.query || "";
    
    // Find method and execute with wrapper
    const method = methodMap.get(toolName);
    const processedResult = await executeToolWithLLMWrapper(method, query);
    
    return {
      tool_call_id: call.id,
      role: "tool" as const,
      name: toolName,
      content: processedResult, // Processed string, not raw data
    };
  });
  
  const toolResults = await Promise.all(toolCallPromises);
  
  // Send processed results back to main model
  const finalResponse = await openai.chat.completions.create({
    model: "gpt-4-turbo-preview",
    messages: [...messages, assistantMessage, ...toolResults],
  });
  
  return finalResponse.choices[0]?.message?.content;
}
```

## Anti-patterns

### Don't Return Raw Tool Results
- ❌ `return JSON.stringify(apiResponse)`
- ✅ `return await executeToolWithLLMWrapper(method, query)`

### Don't Use Expensive Models for Wrapper
- ❌ `model: "gpt-4-turbo-preview"` (too expensive for processing)
- ✅ `model: "gpt-3.5-turbo"` (cheap, fast, sufficient)

### Don't Include Chat History in Wrapper
- ❌ Pass entire chat history to wrapper LLM
- ✅ Only pass query, method info, and raw results

### Don't Skip Query Parameter
- ❌ Tools with no parameters or many parameters
- ✅ Single `query` parameter (natural language)

### Don't Execute Tools Directly
- ❌ `const result = await fetch(method.path)`
- ✅ `const result = await executeToolWithLLMWrapper(method, query)`

## Examples

### Tool Call Flow
```typescript
// 1. User asks: "What's the current price of Bitcoin?"

// 2. Tool selector returns: [getCryptoPrice method]

// 3. Main model calls tool:
{
  name: "getCryptoPrice",
  arguments: '{"query": "Get the current price of Bitcoin in USD"}'
}

// 4. Wrapper executes (mocked):
const rawResult = {
  status: 200,
  data: { price: 45000, currency: "USD", timestamp: "2024-..." }
};

// 5. Wrapper sends to gpt-3.5-turbo:
"Tool: getCryptoPrice
 User Query: 'Get the current price of Bitcoin in USD'
 Result: {...}
 
 Please provide insights..."

// 6. gpt-3.5-turbo returns:
"The current price of Bitcoin is $45,000 USD as of [timestamp]."

// 7. Main model receives processed result and responds to user
```

### Parallel Tool Execution
```typescript
// If main model calls multiple tools
const toolCallPromises = assistantMessage.tool_calls.map(async (call) => {
  return await executeToolWithLLMWrapper(method, query);
});

// Execute all in parallel (faster)
const toolResults = await Promise.all(toolCallPromises);
```

### HTTP Tool Execution (Mocked)
```typescript
// In lib/tool-wrapper.ts
async function executeHttpTool(method: Method): Promise<unknown> {
  console.log(`[tool-wrapper] Mocking HTTP execution for ${method.name}`);
  console.log(`[tool-wrapper]   ${method.httpVerb} ${method.path}`);
  
  // TODO: Replace with actual HTTP client
  return {
    status: 200,
    data: {
      message: `Mock response for ${method.name}`,
      timestamp: new Date().toISOString(),
    },
  };
}
```

## Best Practices

### Model Selection
- Main model: gpt-4-turbo-preview (smart, handles complex queries)
- Wrapper model: gpt-3.5-turbo (cheap, fast, good at summarizing)
- Never use gpt-4 for wrapper (too expensive)

### Query Design
- Main model passes natural language query to tool
- Query describes what user wants to know/do
- Wrapper uses query to contextualize results
- Keep queries focused and specific

### Error Handling
- Always return string from wrapper (never throw)
- Include error messages in returned string
- Log errors with full context
- Main model can handle error messages naturally

### Logging
- Use `[tool-wrapper]` prefix for wrapper logs
- Use `[chat-service]` prefix for orchestration logs
- Log: Entry, HTTP call (mocked), LLM call, results
- Log: Truncated responses for readability

### Performance
- Execute tools in parallel when possible
- Use gpt-3.5-turbo for fast wrapper processing
- Mock HTTP calls don't block (instant response)
- Actual HTTP calls: consider timeout and retries

### Future: HTTP Client Implementation
```typescript
// TODO: Replace mock with actual HTTP client
async function executeHttpTool(method: Method): Promise<unknown> {
  const url = `${method.baseUrl}${method.path}`;
  const options = {
    method: method.httpVerb,
    headers: { "Content-Type": "application/json" },
    // Map method.arguments to request body/query params
  };
  
  const response = await fetch(url, options);
  return await response.json();
}
```

## Type Definitions

### Tool Wrapper Function
```typescript
async function executeToolWithLLMWrapper(
  method: Method,
  query: string
): Promise<string>
```

### Tool Definition
```typescript
{
  type: "function",
  function: {
    name: string,
    description: string,
    parameters: {
      type: "object",
      properties: {
        query: {
          type: "string",
          description: string
        }
      },
      required: ["query"]
    }
  }
}
```

### Method Type
```typescript
type Method = {
  id: string;
  name: string;
  path: string;
  httpVerb: string;
  description: string | null;
  arguments: ToolArgument[];
  returnType: string | null;
  // ...
}
```

## Integration Points

### Chat Service Integration
- `convertMethodsToOpenAITools()`: Converts Method[] to OpenAI tool format
- Tool execution handler: Extracts query, finds method, calls wrapper
- Result aggregation: Sends all processed results to main model

### Tool Selector Integration
- Tool selector returns Method objects
- Methods converted to OpenAI tools with query parameter
- Main model decides when/how to call tools

### Future: HTTP Client Integration
- Replace mock with actual HTTP client
- Map method arguments to request params
- Handle authentication, retries, timeouts
- Support all HTTP verbs (GET, POST, PUT, DELETE, etc.)
