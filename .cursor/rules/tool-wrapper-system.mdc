---
globs: lib/tool-wrapper.ts,lib/chat-service.ts,lib/web-search-service.ts,lib/tavily-client.ts
alwaysApply: false
---

# Tool Wrapper System

## Context

The tool wrapper system intercepts tool calls from the main LLM and wraps each tool execution with web search and LLM processing. The system performs real web searches via Tavily API, then uses LLM to synthesize results into natural language answers.

**Status**: ✅ Fully implemented with Tavily web search integration

## Architecture

### Flow
1. Main model calls tool with `query` parameter
2. Web search service generates search queries from tool context + user query
3. Tavily API performs parallel web searches (1-3 queries per tool)
4. Search results formatted and sent to LLM wrapper (gpt-3.5-turbo)
5. LLM synthesizes real search data into natural language answer
6. Main model receives processed insights based on real web data
7. Tavily request/response data persisted in metadata for debugging

### Key Files
- `lib/tool-wrapper.ts`: Tool execution wrapper with web search + LLM processing
- `lib/web-search-service.ts`: Web search service (query generation, execution, formatting)
- `lib/tavily-client.ts`: Tavily API client singleton
- `lib/chat-service.ts`: Tool conversion and execution handler

## Patterns

### Tool Definition Pattern
```typescript
// In lib/chat-service.ts
export function convertMethodsToOpenAITools(methods: Method[]) {
  return methods.map(method => ({
    type: "function" as const,
    function: {
      name: method.name,
      description: method.description || `Execute ${method.name} tool`,
      parameters: {
        type: "object" as const,
        properties: {
          query: {
            type: "string",
            description: "Natural language query describing what you want to know or do with this tool",
          },
        },
        required: ["query"],
      },
    },
  }));
}
```

### Tool Wrapper Pattern
```typescript
// In lib/tool-wrapper.ts
export async function executeToolWithLLMWrapper(
  method: Method,
  query: string
): Promise<ToolExecutionResult> {
  // 1. Perform web searches via Tavily
  const searchResults = await webSearchService.searchForTool(method, query, {
    maxResultsPerQuery: 5,
    searchDepth: "basic",
    includeAnswer: true,
  });
  
  if (!searchResults.hasResults) {
    return { result: await generateResponseWithoutSearch(method, query) };
  }
  
  // 2. Prepare prompts for LLM synthesis with real search data
  const systemPrompt = `You are a tool execution assistant. 
    Answer the user's query based on REAL web search results.
    Use ONLY information from the provided search results.`;
  
  const userPrompt = `Tool: ${method.name}
    User Query: "${query}"
    Web Search Results: ${searchResults.detailedResults}
    
    Synthesize the search results into a natural language answer.`;
  
  // 3. Call LLM to synthesize search results
  const response = await openai.chat.completions.create({
    model: getModel("toolWrapper"),
    messages: [
      { role: "system", content: systemPrompt },
      { role: "user", content: userPrompt },
    ],
  });
  
  return {
    result: response.choices[0]?.message?.content || "Error",
    tavilyData: searchResults.rawSearchData,
  };
}
```

### Web Search Service Pattern
```typescript
// In lib/web-search-service.ts
export class WebSearchService {
  async searchForTool(
    method: Method,
    userQuery: string,
    options?: { maxResultsPerQuery?: number; searchDepth?: "basic" | "advanced" }
  ): Promise<FormattedSearchResults> {
    // Generate 1-3 search queries from tool context
    const queries = this.generateSearchQueries(method, userQuery);
    
    // Perform parallel searches via Tavily
    const results = await Promise.all(
      queries.map(q => searchWeb(q, options))
    );
    
    // Format for LLM consumption
    return this.formatSearchResults(results);
  }
}
```

### Tool Execution Handler Pattern
```typescript
// In lib/chat-service.ts
if (assistantMessage.tool_calls && assistantMessage.tool_calls.length > 0) {
  // Extract query (generate from args if missing)
  const args = JSON.parse(call.function.arguments);
  let query = args.query || "";
  if (!query && typeof args === "object") {
    query = Object.entries(args)
      .filter(([k]) => k !== "query")
      .map(([k, v]) => `${k}: ${v}`)
      .join(", ");
  }
  
  // Execute with wrapper (returns ToolExecutionResult)
  const toolResult = await executeToolWithLLMWrapper(method, query);
  
  // Store Tavily data in metadata
  toolData.tavilyData = toolResult.tavilyData;
  
  return {
    tool_call_id: call.id,
    role: "tool" as const,
    name: toolName,
    content: toolResult.result, // Synthesized from real web search
  };
}
```

### System Prompt Behavior
The system prompt (`buildSystemPromptWithToolDetails()`) adapts based on tool availability:

**When no tools are available:**
- Instructs LLM to NEVER make up specific data, prices, or market information
- Requires honesty about data limitations
- Encourages explaining concepts, terms, and macroeconomics principles
- Guides users to rephrase queries with different keywords (e.g., "price", "volume", "market cap")
- Provides educational value about market data analysis

**When tools are available but not relevant:**
- Acknowledges that available tools don't match the query
- Explains what tools can do instead
- Helps users understand concepts and terminology
- Encourages rephrasing for better tool matching
- Never makes up data when tools aren't applicable

**When tools are relevant:**
- Uses tools proactively to provide data-driven responses
- Makes intelligent assumptions about parameters
- Presents data with insights, not just raw numbers

## Anti-patterns

### Don't Return Raw Tool Results
- ❌ `return JSON.stringify(apiResponse)`
- ✅ `return await executeToolWithLLMWrapper(method, query)`

### Don't Use Expensive Models for Wrapper
- ❌ `model: "gpt-4-turbo-preview"` (too expensive for processing)
- ✅ `model: "gpt-3.5-turbo"` (cheap, fast, sufficient)

### Don't Include Chat History in Wrapper
- ❌ Pass entire chat history to wrapper LLM
- ✅ Only pass query, method info, and raw results

### Don't Skip Query Parameter
- ❌ Tools with no parameters or many parameters
- ✅ Single `query` parameter (natural language)

### Don't Execute Tools Directly
- ❌ `const result = await fetch(method.path)`
- ✅ `const result = await executeToolWithLLMWrapper(method, query)`

### Don't Make Up Data When Tools Aren't Available
- ❌ Fabricating prices, statistics, or market data when no tools match
- ❌ Pretending tools can answer questions they cannot
- ✅ Acknowledge limitations honestly
- ✅ Explain concepts and guide users to rephrase queries
- ✅ Use tools only when they're relevant to the query

## Examples

### Tool Call Flow
```typescript
// 1. User asks: "What's the current price of Bitcoin?"

// 2. Tool selector returns: [getCryptoPrice method]

// 3. Main model calls tool:
{
  name: "getCryptoPrice",
  arguments: '{"query": "Get the current price of Bitcoin in USD"}'
}

// 4. Wrapper executes (mocked):
const rawResult = {
  status: 200,
  data: { price: 45000, currency: "USD", timestamp: "2024-..." }
};

// 5. Wrapper sends to gpt-3.5-turbo:
"Tool: getCryptoPrice
 User Query: 'Get the current price of Bitcoin in USD'
 Result: {...}
 
 Please provide insights..."

// 6. gpt-3.5-turbo returns:
"The current price of Bitcoin is $45,000 USD as of [timestamp]."

// 7. Main model receives processed result and responds to user
```

### Parallel Tool Execution
```typescript
// If main model calls multiple tools
const toolCallPromises = assistantMessage.tool_calls.map(async (call) => {
  return await executeToolWithLLMWrapper(method, query);
});

// Execute all in parallel (faster)
const toolResults = await Promise.all(toolCallPromises);
```

### Tavily Client Pattern
```typescript
// In lib/tavily-client.ts
export async function searchWeb(
  query: string,
  options?: { maxResults?: number; searchDepth?: "basic" | "advanced" }
): Promise<WebSearchResult | null> {
  const client = getTavilyClient(); // Singleton, requires TAVILY_API_KEY
  if (!client) return null;
  
  return await client.search({ query, ...options });
}
```

## Best Practices

### Model Selection
- Main model: gpt-4-turbo-preview (smart, handles complex queries)
- Wrapper model: gpt-3.5-turbo (cheap, fast, good at summarizing)
- Never use gpt-4 for wrapper (too expensive)

### Query Design
- Main model passes natural language query to tool
- Query describes what user wants to know/do
- Wrapper uses query to contextualize results
- Keep queries focused and specific

### Error Handling
- Always return string from wrapper (never throw)
- Include error messages in returned string
- Log errors with full context
- Main model can handle error messages naturally

### Logging
- Use `[tool-wrapper]` prefix for wrapper logs
- Use `[chat-service]` prefix for orchestration logs
- Log: Entry, HTTP call (mocked), LLM call, results
- Log: Truncated responses for readability

### Performance
- Execute tools in parallel when possible
- Use gpt-3.5-turbo for fast wrapper processing
- Mock HTTP calls don't block (instant response)
- Actual HTTP calls: consider timeout and retries

### Future: HTTP Client Implementation
```typescript
// TODO: Replace mock with actual HTTP client
async function executeHttpTool(method: Method): Promise<unknown> {
  const url = `${method.baseUrl}${method.path}`;
  const options = {
    method: method.httpVerb,
    headers: { "Content-Type": "application/json" },
    // Map method.arguments to request body/query params
  };
  
  const response = await fetch(url, options);
  return await response.json();
}
```

## Type Definitions

### Tool Wrapper Function
```typescript
interface ToolExecutionResult {
  result: string;
  tavilyData?: {
    queries: string[];
    requests: Array<{ query: string; options: {...} }>;
    responses: Array<WebSearchResult | null>;
  };
}

async function executeToolWithLLMWrapper(
  method: Method,
  query: string
): Promise<ToolExecutionResult>
```

### Tool Definition
```typescript
{
  type: "function",
  function: {
    name: string,
    description: string,
    parameters: {
      type: "object",
      properties: {
        query: {
          type: "string",
          description: string
        }
      },
      required: ["query"]
    }
  }
}
```

### Method Type
```typescript
type Method = {
  id: string;
  name: string;
  path: string;
  httpVerb: string;
  description: string | null;
  arguments: ToolArgument[];
  returnType: string | null;
  // ...
}
```

## Integration Points

### Chat Service Integration
- `convertMethodsToOpenAITools()`: Converts Method[] to OpenAI tool format
- Tool execution handler: Extracts query, finds method, calls wrapper
- Result aggregation: Sends all processed results to main model

### Tool Selector Integration
- Tool selector returns Method objects
- Methods converted to OpenAI tools with query parameter
- Main model decides when/how to call tools

### Web Search Integration
- Tavily API for real web search (requires `TAVILY_API_KEY` env var)
- Generates 1-3 search queries per tool execution
- Parallel searches for faster results
- Fallback to LLM-only when search unavailable
- Tavily data persisted in metadata for debugging
- Debug modal shows full request/response data
