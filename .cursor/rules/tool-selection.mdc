---
globs: lib/tool-selector.ts,types/tool-selector.ts,lib/repl/**/*
alwaysApply: false
---

# Tool Selection System

## Context

The tool selection system implements a ReAct-like iterative loop that uses LLM-generated code to explore and filter tools from a large pool (100-200 tools) down to 0-10 relevant tools. The system spawns a real Node.js child process via IPC, allowing variables to persist naturally across iterations without code transformations.

**Status**: ✅ Fully implemented with IPC-based REPL and clean logging

## Architecture

### Two-Stage Process
1. **ToolSelector** (`lib/tool-selector.ts`): Iteratively generates and executes code lines to select relevant tools
2. **Chat Service** (`lib/chat-service.ts`): Uses selected tools with OpenAI to generate responses and execute wrapped tools

### Core Components
- `selectTools()`: Main entry point, spawns child REPL process and implements iterative loop (max 10 steps, can stop early with 0 tools)
- `prepare_initial_context()`: Creates system and user prompts, instructs LLM about REPL persistence and 0-tools capability
- `generate_next_script()`: Uses gpt-4-turbo-preview to generate code lines and thought (including stop with empty tools)
- `executeLines()`: Executes code lines in child REPL via IPC
- `ReplSession` (`lib/repl/ReplSession.ts`): Spawns real Node.js child process, handles IPC communication
- `repl-child` (`lib/repl/repl-child.ts`): Child process with real REPL and META_TOOLS stubs
- `createReplSession()` (`lib/repl/tools.ts`): Factory function to create REPL with META_TOOLS injected
- META_TOOLS: Database query functions using RAG vector search (see `meta-tools-implementation.mdc`)

## Patterns

### System Prompt Requirements
The system prompt instructs the LLM to:
1. **Early termination**: Finish in step 1 or step 2 whenever possible - only use step 3 for truly complex queries
2. **Conversational detection**: Immediately call `finish([])` for greetings/thanks
3. **Full JavaScript power**: Use map, filter, reduce, regex, conditionals, etc.
4. **LLM-controlled filtering**: Choose `threshold` (0.0-1.0) and `top` (result count) based on query complexity
   - Simple queries: threshold 0.4-0.5, top 1-3
   - Medium queries: threshold 0.3-0.4, top 3-5
   - Complex queries: threshold 0.2-0.3, top 5-10
5. **Creative exploration**: Mix search strategies, ask questions early, peek at data
6. **Smart logging**: Be selective - only log counts, slugs, insights (not entire objects)
7. **Variable persistence**: Define once, reuse across iterations
8. **Async awareness**: Always await META_TOOLS calls
9. **Slug extraction**: Use .map(x => x.slug) for subsequent calls
10. **NEVER proceed with empty results**: If search returns 0, GET CREATIVE - try synonyms, broaden search, ask what's available, or use regex on broader results
11. **MANDATORY finish() call**: Every code section MUST end with `await finish(method_slugs)` - this is how tools are returned

Example creative patterns taught to LLM:
```typescript
// Pattern: Simple query with early termination (step 1)
const methods = await get_methods([], [], ["bitcoin price"], 3, 0.4)  // threshold=0.4, top=3
const relevant = methods.filter(m => m.name.toLowerCase().includes("price"))
const check = await ask_to_methods(relevant.map(m => m.slug), "Does this return current price?")
await finish([relevant[0].slug])  // MANDATORY - must call finish()

// Pattern: Ask before drilling down
const apps = await get_apps(["trading", "market"], 5, 0.3)
const response = await ask_to_apps(apps.map(a => a.slug), "Do any support real-time streaming?")
console.log("Real-time:", response.yes ? "YES" : "NO", "-", response.answer.substring(0, 100))

// Pattern: Regex filtering with threshold control
const methods = await get_methods([], [], ["price"], 20, 0.2)  // Lower threshold for broader search
const getters = methods.filter(m => /get|fetch|retrieve/i.test(m.name))
console.log("Found", getters.length, "getter methods")

// Pattern: Handling empty results (CRITICAL)
const classes = await get_classes(appSlugs, ["sentiment"], 5, 0.3)
if (classes.length === 0) {
  console.log("Empty results - trying alternatives...")
  const classes2 = await get_classes(appSlugs, ["opinion", "mood"], 5, 0.2)  // Lower threshold
  if (classes2.length === 0) {
    // Ask what's actually available
    const info = await ask_to_apps(appSlugs, "What classes do you have?")
    console.log(info.answer.substring(0, 150))
  }
}

// Pattern: Smart logging (counts/slugs only)
console.log("Apps:", apps.length, "-", apps.map(a => a.slug))
```

### Iterative Loop Structure
```typescript
const session = createReplSession(); // Spawns child Node.js process
const executionHistory: ExecutionHistoryItem[] = [];
const maxSteps = 3; // Reduced from 10 - encourages early termination

while (step < maxSteps) {
  const { lines, thought } = await generate_next_script(...);
  
  // Check if finish() was called in REPL
  const finishResult = session.getFinishResult();
  if (finishResult !== null) {
    const toolSlugs = finishResult; // Can be empty for conversational queries
    return fetchMethodsBySlugs(toolSlugs);
  }
  
  const result = await executeLines(session, lines.lines);
  executionHistory.push({ lines, thought, result });
  
  // Step 2 prompt encourages finishing if enough info
  // Step 3 is final step with explicit finish() requirement
}
```

### IPC-Based REPL Execution
- **ReplSession** spawns real Node.js child process at start of `selectTools()`
- Child runs actual Node.js REPL (no code transformations needed)
- META_TOOLS communicate via IPC to parent process (single Prisma connection)
- Variables persist naturally in child REPL context
- 30-second timeout per tool call
- Errors don't break the session
- Clean, minimal logging

### Tool Identification
- **Always use slugs**, never IDs or names
- Slugs are required, unique fields on App/Class/Method models
- ToolSelector returns Method objects fetched by slugs

### META_TOOLS Interface
All META_TOOLS are fully implemented with batch support:
- **Search functions**: RAG vector search (get_apps, get_classes, get_methods, get_method_details)
  - Accept optional `threshold` parameter (0.0-1.0) for similarity cutoff - LLM controls filtering
  - Accept `top` parameter for result count - LLM controls result size
  - LLM chooses values based on query complexity (simple: high threshold/small top, complex: low threshold/large top)
- **Ask functions**: LLM-powered Q&A with batch support (ask_to_apps, ask_to_classes, ask_to_methods)
  - Accept arrays of slugs for batch queries
  - Return `{ yes: boolean, no: boolean, answer: string }`
  - Merge context from multiple entities before LLM query
- **Completion function**: `finish(method_slugs: string[])` - MANDATORY call to terminate selection and return tools
  - Can be called at any step (encouraged in step 1-2 for simple queries)
  - Empty array allowed for conversational queries
- All use slugs for entity identification
- See `meta-tools-implementation.mdc` for details

### Smart Logging Philosophy
The agent can work with large datasets but should log selectively:
- **Do log**: Counts, slugs, key insights, yes/no answers
- **Don't log**: Entire objects/arrays (wastes tokens, bloats execution history)
- Principle: Everything logged goes into execution history and costs tokens
- Example good: `console.log("Found:", apps.length, "-", apps.map(a => a.slug))`
- Example bad: `console.log(apps)` or `console.log(JSON.stringify(methods))`

### System Logging
Internal logging with `[tool-selector]` prefix:
- Entry/exit of `selectTools()` with query
- Tool count on completion
- Errors only (no verbose iteration logs)

## Anti-patterns

### Don't Use IDs or Names
- ❌ `get_methods_by_id(methodIds: string[])`
- ✅ `get_method_details(..., method_ids: string[], ...)` (uses slugs internally)

### Don't Create Multiple REPL Sessions
- ❌ Creating new session per iteration spawns new child process
- ✅ Create once, reuse throughout all iterations

### Don't Hardcode Tool Lists
- ❌ Return all tools without filtering
- ✅ Use META_TOOLS to iteratively narrow down based on query

### Don't Skip Execution History
- ❌ Generate code without context from previous iterations
- ✅ Always pass `executionHistory` to `generate_next_script()`

## Best Practices

### Performance & Security
- Max 3 iterations (reduced from 10) - encourages early termination
- **Early termination priority**: Aim to finish in step 1 or step 2
- Simple queries: typically finish in step 1 with appropriate threshold/top values
- Conversational queries: 0 tools in 1 step via `finish([])`
- IPC overhead: ~1-5ms per message
- Single Prisma connection (parent process)
- 30-second timeout per tool call
- Child process isolation (no direct filesystem/network access)
- All inputs validated before DB queries

### Logging
- Minimal: query start, tool count, errors only
- Clean production output

### Testing
- Test conversational queries (should return 0 tools in 1 step)
- Test various query types (specific, broad, ambiguous)
- Monitor logs for errors

## Key Types

- `ThoughtDto`: `{ reasoning?: string }` (removed `stop` and `tools` - now uses `finish()`)
- `ToolSelectorResult`: Returns `{ tools: Method[], reasoning: string, debugData }`
- Method objects fetched by slugs from `finish()` result for downstream processing
- `finish()` function: `(method_slugs: string[]) => Promise<{ success: boolean }>` - terminates selection process
